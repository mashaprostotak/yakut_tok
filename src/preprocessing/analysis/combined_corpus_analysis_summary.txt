# Comprehensive Corpus Analysis for Sakha-Russian NMT
## Analysis of Parallel and Monolingual Datasets

### Introduction

This document synthesizes findings from our analysis of both parallel Sakha-Russian corpora and monolingual Sakha text. The analysis provides insights into the structural and lexical properties of these languages, with particular focus on characteristics relevant to neural machine translation (NMT) development.

### Datasets Analyzed

We examined multiple datasets:

**Parallel Corpora:**
1. **Tatoeba**: 991 sentence pairs of conversational content
2. **Wikimedia**: 7,905 sentence pairs of encyclopedic content

**Monolingual Corpus:**
- **Sakha (yakut_clean.txt)**: 45,246 sentences (25.5 MB), with a 10,000 sentence random sample analyzed

### Key Cross-Dataset Findings

#### Sakha Language Characteristics

**Sentence Structure:**
- **Parallel datasets**: Sakha sentences averaged 3.9 words (Tatoeba) and 12.5 words (Wikimedia)
- **Monolingual dataset**: Sakha sentences averaged 39.2 words with high variability (σ=35.7)
- The substantial difference in sentence length between parallel and monolingual data suggests that parallel corpora may contain shorter, simpler sentences than typical Sakha text

**Word Structure:**
- Across all datasets, Sakha demonstrates characteristics of an agglutinative language
- Monolingual analysis revealed an average word length of 9.43 characters, significantly longer than typical for non-agglutinative languages
- This morphological complexity allows Sakha to encode more information per word than Russian

#### Cross-Linguistic Comparison (Parallel Data)

**Length Ratios (Russian/Sakha):**
- **Character ratio**: Russian text is consistently ~10-12% longer than Sakha (Tatoeba: 1.10, Wikimedia: 1.12)
- **Word ratio**: Russian consistently uses more words than Sakha (Tatoeba: 1.75, Wikimedia: 1.54)
- The higher word ratio compared to character ratio confirms Sakha's agglutinative nature, packing more information into fewer words

**Domain Variation:**
- Wikimedia (encyclopedic) sentences are substantially longer than Tatoeba (conversational) sentences in both languages
- Length ratio variability is higher in Wikimedia (σ=0.70 for word ratio) than Tatoeba (σ=0.53), suggesting more complex translation relationships in encyclopedic content

### Detailed Monolingual Findings

#### Vocabulary Statistics

- **Size**: 113,191 unique words from a 391,989 word sample (diversity ratio: 0.2888)
- **Frequency distribution**: Follows typical Zipfian distribution
  - 50% text coverage with ~1,060 most frequent words
  - 90% text coverage requires ~25,000 most frequent words
- **Hapax legomena**: 71.05% of vocabulary occurs only once, indicating significant lexical richness and data sparsity

#### Monolingual Sentence Structure

- **Length distribution**: Right-skewed with median (29.0 words) lower than mean (39.2 words)
- **Variability**: High standard deviation (35.7 words) indicates considerable sentence length variation
- **Extremes**: Range from 2 to 1,024 words, with longest sentences potentially representing complex multi-clause structures

### Detailed Parallel Corpus Findings

#### Tokenization Comparison

**Example from Tatoeba:**
```
Sakha: "Олохпор биирдэ үчүгэйи оҥоробун... Туһата суох."
Tokens: ['Олохпор', 'биирдэ', 'үчүгэйи', 'оҥоробун...', 'Туһата', 'суох.']

Russian: "Один раз в жизни я делаю хорошее дело... И оно бесполезно."
Tokens: [Один, раз, в, жизни, я, делаю, хорошее, дело, ..., И, оно, бесполезно, .]
```

This demonstrates how Russian requires more words to express the same content as Sakha, with the latter utilizing its agglutinative structure to encode more information per word.

### Implications for NMT Development

#### Architecture Considerations

1. **Asymmetric Model Design**: The consistent word ratio difference suggests NMT architectures should account for the structural asymmetry between languages:
   - Sakha→Russian: Models may need to generate approximately 1.5-1.7 times more words than in the source
   - Russian→Sakha: Models must learn to compress information into fewer, morphologically complex words

2. **Context Window Size**: The long average sentence length in monolingual Sakha (39.2 words) suggests that adequate context window sizes are essential for capturing full semantic meaning

#### Preprocessing Strategies

1. **Tokenization Approach**: 
   - **For Sakha**: Subword tokenization methods (BPE, SentencePiece) are likely essential due to:
     - Agglutinative morphology
     - Large vocabulary size (113,191 unique words in sample)
     - Long average word length (9.43 characters)
   - **For Russian**: Standard tokenization with razdel, as used in the analysis, appears appropriate

2. **Vocabulary Considerations**:
   - The high vocabulary diversity and hapax legomena percentage (71.05%) in Sakha suggests open-vocabulary approaches are necessary
   - Shared subword vocabularies between languages may help address data sparsity

#### Training Data Strategy

1. **Domain Adaptation**: The significant differences between conversational (Tatoeba) and encyclopedic (Wikimedia) content indicate domain-specific approaches may be beneficial

2. **Data Augmentation**: Given the relatively small parallel corpora compared to the lexical richness of Sakha, data augmentation techniques should be explored

3. **Monolingual Pretraining**: The availability of larger monolingual data compared to parallel data suggests potential benefits from:
   - Self-supervised pretraining
   - Back-translation to create synthetic parallel data
   - Multilingual transfer learning from related Turkic languages

### Conclusion

This comprehensive analysis reveals consistent patterns in both parallel and monolingual data that have direct implications for NMT development. The most significant finding is the structural asymmetry between Sakha and Russian, with Sakha's agglutinative nature allowing it to encode more information in fewer, longer words.

The vocabulary and sentence structure analysis of the monolingual Sakha corpus further confirms the need for specialized preprocessing and modeling approaches to handle its morphological complexity and lexical richness.

These insights provide a solid empirical foundation for making informed decisions about architecture design, tokenization strategies, and training approaches for a Sakha-Russian neural machine translation system. 